@article{schick2020exploiting,
	title={Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference},
	author={Timo Schick and Hinrich Sch√ºtze},
	journal={Computing Research Repository},
	volume={arXiv:2001.07676},
	url={http://arxiv.org/abs/2001.07676},
	year={2020}
}

@article{radford2019language,
	title={Language Models are Unsupervised Multitask Learners},
	author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year={2019}
}

@inproceedings{radford2018improving,
	title={Improving Language Understanding by Generative Pre-Training},
	author={Alec Radford and Karthik Narasimhan},
	year={2018}
}

@article{devlin2019bert,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
	author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	year={2019},
	eprint={1810.04805},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{liu2019roberta,
	title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
	author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year={2019},
	eprint={1907.11692},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{44873,
title	= {Distilling the Knowledge in a Neural Network},
author	= {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
year	= {2015},
URL	= {http://arxiv.org/abs/1503.02531},
booktitle	= {NIPS Deep Learning and Representation Learning Workshop}
}

@inproceedings{howard-ruder-2018-universal,
    title = "Universal Language Model Fine-tuning for Text Classification",
    author = "Howard, Jeremy  and
      Ruder, Sebastian",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1031",
    doi = "10.18653/v1/P18-1031",
    pages = "328--339",
    abstract = "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
}

@misc{mccann2018natural,
      title={The Natural Language Decathlon: Multitask Learning as Question Answering}, 
      author={Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
      year={2018},
      eprint={1806.08730},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{puri2019zeroshot,
      title={Zero-shot Text Classification With Generative Language Models}, 
      author={Raul Puri and Bryan Catanzaro},
      year={2019},
      eprint={1912.10165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{qian-yu-2019-domain,
    title = "Domain Adaptive Dialog Generation via Meta Learning",
    author = "Qian, Kun  and
      Yu, Zhou",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1253",
    doi = "10.18653/v1/P19-1253",
    pages = "2639--2649",
    abstract = "Domain adaptation is an essential task in dialog system building because there are so many new dialog tasks created for different needs every day. Collecting and annotating training data for these new tasks is costly since it involves real user interactions. We propose a domain adaptive dialog generation method based on meta-learning (DAML). DAML is an end-to-end trainable dialog system model that learns from multiple rich-resource tasks and then adapts to new domains with minimal training samples. We train a dialog system model using multiple rich-resource single-domain dialog data by applying the model-agnostic meta-learning algorithm to dialog domain. The model is capable of learning a competitive dialog system on a new domain with only a few training examples in an efficient manner. The two-step gradient updates in DAML enable the model to learn general features across multiple tasks. We evaluate our method on a simulated dialog dataset and achieve state-of-the-art performance, which is generalizable to new tasks.",
}

@InProceedings{10.1007/978-3-030-32381-3_16,
author="Sun, Chi
and Qiu, Xipeng
and Xu, Yige
and Huang, Xuanjing",
editor="Sun, Maosong
and Huang, Xuanjing
and Ji, Heng
and Liu, Zhiyuan
and Liu, Yang",
title="How to Fine-Tune BERT for Text Classification?",
booktitle="Chinese Computational Linguistics",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="194--206",
abstract="Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.",
isbn="978-3-030-32381-3"
}


