@article{schick2020exploiting,
	title={Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference},
	author={Timo Schick and Hinrich Sch√ºtze},
	journal={Computing Research Repository},
	volume={arXiv:2001.07676},
	url={http://arxiv.org/abs/2001.07676},
	year={2020}
}

@article{radford2019language,
	title={Language Models are Unsupervised Multitask Learners},
	author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year={2019}
}

@inproceedings{radford2018improving,
	title={Improving Language Understanding by Generative Pre-Training},
	author={Alec Radford and Karthik Narasimhan},
	year={2018}
}

@article{devlin2019bert,
	title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
	author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
	year={2019},
	eprint={1810.04805},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{liu2019roberta,
	title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
	author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year={2019},
	eprint={1907.11692},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}