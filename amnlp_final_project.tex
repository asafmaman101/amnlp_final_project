%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\graphicspath{ {./figs/} }
 
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Multi-Task Patterns Training for Task Generalization}

\author{Asaf Maman \\
  Blavatnik School of Computer Science, Tel Aviv University \\
  \texttt{asafmaman@mail.tau.ac.il} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
In recent years, appearance of large language models paved the way to prompt such model to extract predictions.
The idea behind this mechanism is to provide information about the task and use the model's general knowledge for a specific use.
This method appeard to be relatively successful and reveals many new options for potential use.
One approach for this subject is reformulating inputs as cloze-style phrases and leave one mask token to be used to derive classification prediction.
The restructured sequence and the tokens corresponding to each class label are called patten-verbalizer pair (PVP) and are used as task description.
Evaluating predictions this way enables us to use one language modeling head for multiple tasks with no need to initialize and train task specific weights.
In order to learn new task, the only change that needs to be done is chosing PVP for the new task. 
Multi-tasking training raises several question that I aim to answer in this project.
I found evidences that training models for multi-tasking, when done write, can be used without harming each task's performance.
In addition, when training on semantically similar tasks, multi-task training can, to some extent, generalize to unseen tasks.
\end{abstract}


\section{Introduction}

When handling classification tasks, the common method is to fine-tune the model together with a task-specific classification head.
One alternative way to deal with classification tasks is to reformulate the input examples as cloze-style phrases.
Each example is plugged into a fixed sentence structure, called \textit{pattern}, containing exactly one mask token.
The prediction is then determined according to scores the model assigns to a subset of tokens, called \textit{verbalizers}, each associated with one of the classes.
This mechanism reveals a few adventages over conventional classification methods.
First, it provides some sense of task description encoded in the \textit{pattern-verbalizer pair} (a.k.a. PVP).
This way the model gets information about the task and may be able to infer it from the PVP.
This can enable the model to adopt itself to new tasks without a need for large batch of new-task examples.
%This method utilizes the language modeling capabilities and may be able to infer the task from the PVP.

Another advantage is that training one language modeling head can be used for multiple tasks at once, without a need to train separate model for each.
Finally, making decisions using MLM predictions leverages the language modeling knowlaged attained during pre-training and use it for classification.
%This facilitates more versatile use of the model but also leverages the pre-trained language modeling knowledge for the use of classification.

In this project I would like to investigate how masked language models performance are effected when trained on multiple classification tasks.
In addition, I would like to examine whether providing task description in the form of PVP can help MLMs generalize to \textit{unseen tasks}.
Finally, I will test these questions in few-shot setting using PET pipeline.

% and makes decisions according to the scores the model assigns to each class's specified token,
%which is called \textit{verbalizer}.
%Formatting the inputs into task specific pattern and choosing representative words for classes can be seen as providing task description together with the input.
%Pre-trained language models can handle various classification tasks using fine-tuned classification head.

\input{single-on-train-set-summary}





\section{Method}

\begin{figure}[b]
	\centering
	\includegraphics[width=\linewidth]{mlm_classification}
	\caption{Classification via prompting MLM with PVP}
	\label{normal_case}
\end{figure}

In general, using \textit{pattern-verbalizer pair} (PVP) can be formalized as follows: for a given masked language model $M$, vocabulary $V$ and set of labels $\mathcal{L}$, we define a pattern $P(\textbf{\text{x}})$ that takes as input the raw sample $\textbf{x}\in{}V^*$ and outputs a phrase $P(\textbf{x})\in{}V^*$ that contains exactly one mask token.
For each label in $\mathcal{L}$ we define the set of \textit{verbalizers} using the function $v:\mathcal{L}\rightarrow{}V$ that maps each label to a corresponding token.

We will apply the pattern $P(\textbf{x})$ on each input $\textbf{x}$ that we wish to evaluate.
Then we will use $M$ to process it and recieve the score it assigns to each token $t\in{}V$ at the masked token position.
We denote this unnormalized score as $M(t, \textbf{s})$.
Then we'll calculate the unnormalized score for each class label $l\in{}\mathcal{L}$, that is, $M(v(l) | P(\textbf{x}))$ and denote it by $s_{\text{\textbf{p}}}(l|\textbf{x})$ as the score our language model gives to class label $l$ providing with some PVP $\text{\textbf{p}}=(P, v)$.
Finally we normalize and calculate the probability distribution over the labels using softmax:
\[
	q_\text{\textbf{p}}(l|\textbf{x}) = \frac{e^{s_{\text{\textbf{p}}}(l|\textbf{x})}}{\sum_{l^{'}\in{}\mathcal{L}}e^{s_{\text{\textbf{p}}}(l^{'}|\textbf{x})}}
\]
We then use the cross-entropy between $q_\text{\textbf{p}}(l|\textbf{x})$ and the true one-hot encoded label to calculate the loss over all examples. 

We manually choose three PVPs for each task in a set of tasks $\{A_1, A_2, \dots, A_k\}$ and use them in the following experiments.


\section{Datasets}

\subsection{AG's News}
A dataset for text categorization which contains 120K train samples and 7.6K test samples.
It contains articles formatted as title and text body.
The task is to classify the news to four categories: (1) World, (2) Sports, (3) Business, (4) Science/Tech.

\subsection{Yahoo}
Text categorization dataset. It contains 1.46M pairs of questions and answers.
The train set contains 1.4M samples and test set 60K samples.
The task is to classify each pair to its subject out of 10 possible options: (1) Society (2) Science (3) Health (4) Education (5) Computer (6) Sports (7) Business (8) Entertainment (9) Relationship (10) Politics.

\subsection{Yelp}
Yelp Reviews dataset task is to classify a review for a 5-star rating.
It contains 650K train samples and 50K test samples.
There is another version for that task, while the 5-star scale named "Yelp Full", the second task is to predict whether the review sentiment was positive or negative.
The second task is named "Yelp Polarity".

\subsection{MNLI}
The MNLI dataset contains pairs of sentences to classify between three possible NLI labels: (1) entailment (2) natural (3) contradiction. 
The dataset has 392K train and 9.8K test samples. 

\subsection{Quora Question Pairs}
QQP contains 363K train samples and 40K test samples.
The task is to predict whether two sentences are paraphrases one of the other.

\subsection{Patterns}
The patterns for each task are presented in Appendix~\ref{apx:patterns}.


\section{Experiments}

I conducted several experiments to test the questions raised above.
For each task, a set of PVPs was manually chosen, the PVP where taken from \citet{schick2020exploiting} for tasks that apper in their work.
For the additional tasks, the PVPs were chosen manually by human common sense. 

Each setting that will be described below was trained and evaluated on each PVP separatly.
To provide further analysis and possibly more insights about the behavior of the method, I evaluated each model not only on the PVP that it was trained on.

\subsection{Unsupervised Baselines}
To see how the fine-tuning influences each tasks performance, I evaluated as baseline the zero-shot performance of the pre-trained model with each PVP.

\subsection{Single Task Training}
Firsly, to see how our model perform on each task separatly, I trained and evaluated on single task.
The performance of supervised training using PVPs succeed to match the matching results with classification head showed in \citet{schick2020exploiting}.

In addition, when evaluating models trained on single task on different tasks, few of them showed improvement meaning some sense of generalization.
In contrast, there was pairs of tasks that training on one deteriorated the performance on another.
One can notice that training on AGnews and Yahoo! increased the performance when evaluating on the other task, compared to the unsupervised baseline.
The 

\subsection{Multi-Task Training}

\input{multi-on-train-set-summary}

To test how training on multi-task will generalize to unseen tasks, I combined the training set of various tasks ans trained a model on multiple tasks.
Each time I omitted one task from the set of all tasks and trained a model on all the others.
The missing task was the one that I aimed to generalize to.
I evaluated all models on each task, the results are reported in Table~\ref{tab:multi-on-train-set-summary}.


\subsection{PET Multi-Task Training}
Fig~\ref{tab:multi-use-logits-summary}


\input{multi-use-logits-summary}


\section{Conclusion}


\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\newpage
\appendix

\section{Appendices}
\label{sec:appendix}

\subsection{Implementation Details}

\subsection{Patterns}
\label{apx:patterns}

\input{multi-use-logits-full}

\input{single-on-train-set}







\end{document}
