\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{float}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{tabularx}
%\usepackage{geometry}
%\usepackage{pdfpages}
\graphicspath{ {./figs/} }
 
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}
\newcommand\todo[1]{\textcolor{red}{#1}}

\title{Multi-Task Patterns Training for Task Generalization}

\author{Asaf Maman \\
  Blavatnik School of Computer Science, Tel Aviv University \\
  \texttt{asafmaman@mail.tau.ac.il} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
In recent years, appearance of large language models paved the way to calculate predictions via prompts.
One key feature of this mechanism is the ability to provide information about the task.
One prompting approach is reformulating inputs as cloze-style phrases and deriving predictions accorting the the probabilites the model assigns.
The restructured sequence and the tokens corresponding to each class label are called patten-verbalizer pair (PVP) and are used as task description.
Evaluating predictions this way enables us to use one language modeling head for multiple tasks with no need to initialize and train task specific weights.
In order to learn new task, the only change that needs to be done is chosing PVP for the new task. 
Multi-tasking training raises several question that I aim to answer in this project.
I found evidences that training models for multi-tasking, when done write, can be used without harming each task's performance.
In addition, when training on semantically similar tasks, multi-task training can, to some extent, generalize to unseen tasks.
\end{abstract}


\section{Introduction}

When handling classification tasks, the common method is to fine-tune the model together with a task-specific classification head.
Recenly, when large pre-trained language models such as GPT \citep{radford2018improving}, BERT \citep{devlin2019bert} and RoBERTa \citep{liu2019roberta}, utilizing the knowledge attained from pre-training via prompting has become feasible.
Providing task description together with examples has proved to be useful \citep{radford2019language} even for zero-shot and few-shot inference.
This also envailed an alternative way to deal with classification tasks: reformulating the input examples as cloze-style phrases.
Each example is plugged into a fixed sentence structure, called \textit{pattern}, containing exactly one mask token.
The prediction is then determined according to scores the model assigns to a subset of tokens, called \textit{verbalizers}, each associated with one of the classes.
This mechanism reveals a few adventages over conventional classification methods.

First, as mentioned before, it provides some sense of task description encoded in the \textit{pattern-verbalizer pair} (a.k.a. PVP).
This way the model gets information about the task and may be able to infer it from the PVP.
This can enable the model to adopt itself to new tasks without a need for large batch of new-task examples.
Another advantage is that training one language modeling head can be used for multiple tasks at once, without a need to train separate model for each.
Finally, making decisions using MLM predictions leverages the language modeling knowledge attained during pre-training and use it for classification.

In this project I would like to investigate how masked language models (MLMs) performance are effected when trained on multiple classification tasks.
In addition, I would like to examine whether providing task description in the form of PVP can help MLMs generalize to \textit{unseen tasks}.
Finally, I will test these questions in few-shot setting using PET pipeline.

% SINGLE TASK TRAINING ON TRAIN SET SUMMARY TABLE
\input{single-on-train-set-summary}

\section{Method}

\begin{figure}[b]
	\centering
	\includegraphics[width=\linewidth]{mlm_classification}
	\caption{Classification via prompting MLM with PVP}
	\label{normal_case}
\end{figure}

In general, using \textit{pattern-verbalizer pair} (PVP) can be formalized as follows: for a given masked language model $M$, vocabulary $V$ and set of labels $\mathcal{L}$, we define a pattern $P(\textbf{\text{x}})$ that takes as input the raw sample $\textbf{x}\in{}V^*$ and outputs a phrase $P(\textbf{x})\in{}V^*$ that contains exactly one mask token.
For each label in $\mathcal{L}$ we define the set of \textit{verbalizers} using the function $v:\mathcal{L}\rightarrow{}V$ that maps each label to a corresponding token.

We will apply the pattern $P(\textbf{x})$ on each input $\textbf{x}$ that we wish to evaluate.
Then we will use $M$ to process it and recieve the score it assigns to each token $t\in{}V$ at the masked token position.
We denote this unnormalized score as $M(t, \textbf{s})$.
Then we'll calculate the unnormalized score for each class label $l\in{}\mathcal{L}$, that is, $M(v(l) | P(\textbf{x}))$ and denote it by $s_{\text{\textbf{p}}}(l|\textbf{x})$ as the score our language model gives to class label $l$ providing with some PVP $\text{\textbf{p}}=(P, v)$.
Finally we normalize and calculate the probability distribution over the labels using softmax:
\[
	q_\text{\textbf{p}}(l|\textbf{x}) = \frac{e^{s_{\text{\textbf{p}}}(l|\textbf{x})}}{\sum_{l^{'}\in{}\mathcal{L}}e^{s_{\text{\textbf{p}}}(l^{'}|\textbf{x})}}
\]
We then use the cross-entropy between $q_\text{\textbf{p}}(l|\textbf{x})$ and the true one-hot encoded label to calculate the loss over all examples. 

We manually choose three PVPs for each task in a set of tasks $\{A_1, A_2, \dots, A_k\}$ and use them in the following experiments.



\section{Datasets}

I chose the following datasets to evaluate performence on.

\vspace{8pt}
\noindent \textbf{AG's News} \quad
A dataset for text categorization which contains 120K train samples and 7.6K test samples.
It contains articles formatted as title and text body.
The task is to classify the news to four categories: (1) World, (2) Sports, (3) Business, (4) Tech.

\vspace{8pt}
\noindent \textbf{Yahoo} \quad
Text categorization dataset. It contains 1.46M pairs of questions and answers.
The train set contains 1.4M samples and test set 60K samples.
The task is to classify each pair to its subject out of 10 possible options: (1) Society (2) Science (3) Health (4) Education (5) Computer (6) Sports (7) Business (8) Entertainment (9) Relationship (10) Politics.

\vspace{8pt}
\noindent \textbf{Yelp} \quad
Yelp Reviews dataset task is to classify a review for a 5-star rating.
It contains 650K train samples and 50K test samples.
There is another version for that task, while the 5-star scale named "Yelp Full", the second task is to predict whether the review sentiment was positive or negative.
The second task is named "Yelp Polarity".

\vspace{8pt}
\noindent \textbf{MNLI} \quad
The MNLI dataset contains pairs of sentences to classify between three possible NLI labels: (1) entailment (2) natural (3) contradiction. 
The dataset has 392K train and 9.8K test samples. 

\vspace{8pt}
\noindent \textbf{Quora Question Pairs} \quad
QQP contains 363K train samples and 40K test samples.
The task is to predict whether two sentences are paraphrases one of the other.

\subsection{Patterns}
The patterns for each task are presented in Appendix~\ref{apx:patterns}.


\section{Experiments}
\label{sec:experiments}

I conducted several experiments to test the questions raised above.
For each task, a set of PVPs was manually chosen, the PVP where taken from \citet{schick2020exploiting} for tasks that appear in their work.
For the additional tasks, the PVPs were chosen manually by human common sense. 

Each setting that will be described below was trained and evaluated on each PVP separatly.
To provide further analysis and possibly more insights about the behavior of the method, I evaluated each model not only on the PVP that it was trained on.

\subsection{Experimental Setup}
\label{sub-sec:experimental-setup}

All the additional implementation details and code can be found in this github repository\footnote{\url{https://github.com/asafmaman101/amnlp_final_project_code}}.

\subsection{Unsupervised Baselines}
To see how the fine-tuning influences each tasks performance, I evaluated as baseline the zero-shot performance of the pre-trained model with each PVP.

\subsection{Results}

\noindent\textbf{Single-task Training}\quad
Firsly, to see how our model perform on each task separatly, I trained and evaluated on single task.
The performance of supervised training using PVPs succeed to match the matching results with classification head showed in \citet{schick2020exploiting}.

In addition, when evaluating models trained on single task on different tasks, few of them showed improvement meaning some sense of generalization.
In contrast, there was pairs of tasks that training on one deteriorated the performance on another.
The results for this experiments are reported in Table~\ref{tab:single-on-train-set-summary}.
Extended report can be found in Table~\ref{tab:single-on-train-set} in Appendix~\ref{apx:full-results-report}.

One can notice that training on AGnews and Yahoo! increased the performance when evaluating on the other task, compared to the unsupervised baseline.
An important artifact regarding this pair of datasets is that although their distribution is different and the set of classes is different, they share two class labels that take part in both: Sports and Business.
Our method does facilitate classifing while being agnostic to the number of classes.
However, in this pair of tasks we cannot still conclude the model learned to generalize to unseen tasks, because they are essentially too close one to another.

For some tasks, the performance of different PVPs performed very differently.
For example how QQP performed on Yelp Polarity task, when looking on the extended results (that can be found in Table~\ref{tab:single-on-train-set} in Appendix~\ref{apx:full-results-report} we can see that two of the PVPs scored roughly 88\% but one PVP got ~49\%.
This emphesises the importance of PVP choice for generalization.

\vspace{8pt}
\noindent\textbf{Multi-Task Training}\quad
\input{multi-on-train-set-summary}
To test how training on multi-task will generalize to unseen tasks, I combined the training set of multiple tasks and trained a model all of them together.
Each time I omitted one task from the training tasks and fit a model on all the others.
The missing task was the one that I aimed to generalize to.
I evaluated all models on each task separatly, the results are reported in Table~\ref{tab:multi-on-train-set-summary}.

When omitting agnews, the model succeeded to get 82.0\% accuracy, that is and improvement over the baseline.
This means that training on the other tasks benefits with this task and in fact generalizes.
Although, when comparing this score to the single task traing we can see almost the same result when evalutaing a model that was trained on yahoo to agnews.
This points out on the fact that most of the knowledge learned in training got from the Yahoo task and not from the remaining tasks.
Here is worth mentioning that both yahoo

To some extent, training a model for multi-tasks benefits with performance on unseen tasks.
It still seem that the ability of the model to infer the task according to the PVP is limited to semantically close tasks.
While from the results I didn't observe option to train a model that generalize to any task, regardless of the tasks it was trained on, I think that this limitation is somewhat expected.
Moreover, as PVP is the key for the model to infer the task, its choice might be crucial for success in generalizing to unseen tasks.
Therefore, one might suggest better ways to generate PVP in place of manually choosing them.
I think this adjustment can improve generalization performance a lot.


\vspace{8pt}
\noindent\textbf{PET Multi-Task Training}\quad
\input{multi-use-logits-summary}
In \citet{schick2020exploiting}, the authors presented a method for training in few-shot regime called PET.
As reported in the paper, their method outperformed other supervised baselines.

As an extention for the multi-tasking training done previously in this project, I used PET to train multi-task models with few examples for each task.
I wanted to examine whether the multi-task training will improve performace while exploiting the ability to increase dataset size by taking samples from different tasks.

The results for this setup can be found in Table~\ref{tab:multi-use-logits-summary}.
For each task, I used 1000 examples from each task to train ensemble of models.
I then used a soft-labeled dataset of size 20K to train one classifier, but here, differently from \citet{schick2020exploiting}, I used PVPs also for the classifier.
That is, instead of training classifier with classification head for a specific task, I used different PVP for each task.

The results are mixed, when omitting AG News or Yahoo, the model succeeds to generalize at some level (Alghough, this insight is rather biased as mentioned in \todo{(reference)}).
For Yelp Full and MNLI, the model fails to perform without seeing the task.
On the other hand, the lack in performance does not seem to stem from the small amount of example.
This can be seen when comparing the results to those in Table~\ref{tab:multi-on-train-set-summary}.
In comparison to supervised results, PET performs slightly (but not significantly) worse.
This result is not suprising and mostly follows the results revealed in \citet{schick2020exploiting}, but testing here shows that it can also get the benfits of multi-tasking.

\section{Conclusion}


\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\newpage
\appendix


\section{Appendices}
\label{sec:appendix}

\subsection{Implementation Details}
The following hyper parameters were chosen for the experiments in Section~\ref{sec:experiments}.
I chose the RobertaLarge as the model to be used across all experiments.
This choice was done in order to be able to compare results with \citet{schick2020exploiting}.
We trained the model with a max sequence size of 256 tokens.
The batch size was chosen so that it will fit into 11GB GPU infrastucture, this led to a batch size of 2. 
In order to simulate larger batch sizes we accumulated gradients for 8 steps to have effective batch size of 32.
Learning rate was chosen to be 1e-5, we trained each model for 3 epochs on the whole train set.
Temperature was tuned to value of 2, weight decay to 0.01, and adam epsilon to 1e-8.

\subsection{Patterns}
\label{apx:patterns}
I chose the following datasets to evaluate performence on.

\vspace{8pt}
\noindent \textbf{AG's News} \quad
Each sample $\textbf{x}$ in the dataset consists of title and text body.
That is $\textbf{x}=(a, b)$ where $a$ and $b$ represents the title and text body respectively.
For the experiments, I used the following three patterns:
\begin{table}[H]
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\textwidth}{cc}
		 $P_1(x)=$ \_\_\_\_: $a$ $b$ & $P_2(x)=$ \_\_\_\_ News: $a$ $b$ \\
		 $P_3(x)=$ $a$ (\_\_\_\_) $b$ &  \\
	\end{tabularx}
\end{table}
The verbalizers used for each class were World, Sports, Business and Tech corresponding to the dataset categories.

\vspace{8pt}
\noindent \textbf{Yahoo} \quad
Each sample $\textbf{x}$ in the dataset consists of question and answer.
That is $\textbf{x}=(a, b)$ where $a$ and $b$ represents the question and answer respectively.
For the experiments, I used the following three patterns:
\begin{table}[H]
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\textwidth}{cc}
		$P_1(x)=$ \_\_\_\_: $a$ $b$ & $P_2(x)=$ \_\_\_\_ Question: $a$ $b$ \\
		$P_3(x)=$ $a$ (\_\_\_\_) $b$ &  \\
	\end{tabularx}
\end{table}
The verbalizers used for each class were "Society", "Science", "Health", "Education", "Computer", "Sports", "Business", "Entertainment", "Relationship" and "Politics", corresponding to the dataset categories.

\vspace{8pt}
\noindent \textbf{Yelp} \quad
Each sample $\textbf{x}$ in the dataset consists of a review $a$.
For the experiments, I used the following three patterns:
\begin{table}[H]
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\textwidth}{l}
		$P_1(x)=$It was \_\_\_\_. $a$ \\
		$P_2(x)=$ $a$. All i n all, it was \_\_\_\_. \\
		$P_3(x)=$ Just \_\_\_\_! $a$  \\
	\end{tabularx}
\end{table}
The Yelp Full and Yelp Polarity were differentiated by the verbalizers used in each task.
For Yelp Full, the 1-5 rating scale was affiliated with "terrible", "bad", "okay", "good", "great" verbalizers.
Yelp Polarity was flatten to "bad" and "good" only.

\vspace{8pt}
\noindent \textbf{MNLI} \quad
Each sample $\textbf{x}$ in the dataset consists of two sentences $a$ and $b$.
The three patterns used for the experiments were:
\begin{table}[H]
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\textwidth}{cc}
		$P_1(x)=$"$a$"? \_\_\_\_, "$b$" & $P_2(x)=$$a$? \_\_\_\_, $b$ \\
		$P_3(x)=$"$a$"? \_\_\_\_, "$b$" & \\
	\end{tabularx}
\end{table}
And the verbalizers used for each were "Wrong", "Right" and "Maybe" for Contradiction, Entailment and Natural classes respectively.
$P_3$ differs from $P_1$ by its verbalizers, it was used with "No", "Yes" and "Maybe" instead.

\vspace{8pt}
\noindent \textbf{Quora Question Pairs} \quad
Each sample $\textbf{x}$ in the dataset consists of two sentences $a$ and $b$.
The three patterns used for the experiments were:
\begin{table}[H]
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\textwidth}{cc}
		$P_1(x)=$"$a$"? \_\_\_\_, "$b$" & $P_2(x)=$$a$? \_\_\_\_, $b$ \\ 
		$P_3(x)=$"$a$"? \_\_\_\_, "$b$" & \\
	\end{tabularx}
\end{table}
And the verbalizers used for each were "Wrong" and "Right" for sentences that are not paraphrases and are paraphrases respectively.
$P_3$ differs from $P_1$ by its verbalizers, it was used with "No" and "Yes" instead.

\subsection{Full Results Report}
\label{apx:full-results-report}
The exact score for each single experiment is describe in and Table~\ref{tab:single-on-train-set} and Table~\ref{tab:multi-use-logits-full}.
\input{multi-use-logits-full}
%\input{single-on-train-set}
\input{single-on-train-set-horizontal}
%\includegraphics{single-on-train-set-horizontal copy}


\end{document}
